<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta http-equiv="Content-Style-Type" content="text/css">
  <title></title>
  <meta name="Generator" content="Cocoa HTML Writer">
  <meta name="CocoaVersion" content="1138.23">
  <style type="text/css">
    p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 14.0px Verdana}
    p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 14.0px Verdana; min-height: 17.0px}
    li.li1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 14.0px Verdana}
    span.s1 {color: #027ac6}
    ul.ul1 {list-style-type: disc}
  </style>
</head>
<body>


<p class="p1"><a href="http://workshop2016.iwslt.org/"><span class="s1">IWSLT 2016</span></a> human evaluation data is available <a href="javascript:download('2016-01','subeval_files/IWSLT16-HE-RELEASE.zip');"><span class="s1">here</span></a>.<span class="Apple-converted-space"> </span></p>

<p class="p1"> 
Human evaluation was carried out on primary runs submitted by participants to two of the official MT TED tasks, namely English-German (EnDe) and English-French (EnFr). 
</p>

<p class="p1">
The human evaluation (HE) dataset created for each MT task was a subset of the official test set (tst2015). Both the EnDe and EnFr tst2015 datasets are composed of 12 TED Talks, and the first 56% of each talk was selected. The resulting HE sets are identical and composed of 600 segments, each corresponding to around 10,000 words.
</p>

<p class="p1"> 
Human evaluation was based on Post-Editing, i.e. the manual correction of the MT system output, which was carried out by professional translators.
</p>

<p class="p1"> 
Five runs were evaluated for each of the two tasks, and the resulting evaluation data consist of five new reference translations for each of the sentences composing the two HE sets.
</p>

<p class="p1">For further information see:</p>
<p class="p1">
M. Cettolo, J. Niehues, S. Stüker, L. Bentivogli, R. Cattoni, M. Federico.<br><a href="http://workshop2016.iwslt.org/63.php"><span class="s1">The IWSLT 2016 Evaluation Campaign</span></a>.<br>
In Proceedings of the International Workshop on Spoken Language Translation (IWSLT-2016), Seattle (US-WA), 8-9 December 2016.
</p>

<p class="p1">
A detailed analysis of both EnDe and EnFr human evaluation data was carried out with the aim of understanding in what respects Neural MT provides better translation quality than Phrase-Based MT. The results of this analysis are presented in:
</p>

<p class="p1">L. Bentivogli, A. Bisazza, M. Cettolo, M. Federico.<br>
<a href="http://dx.doi.org/10.1016/j.csl.2017.11.004">
<span class="s1">
"Neural versus phrase-based MT quality: An in-depth analysis on English-German and English-French"
</span></a>.<br>
In Computer Speech & Language (2018)
</p>
<br>

</body>
</html>
